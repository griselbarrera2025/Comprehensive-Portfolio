{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/griselbarrera2025/Comprehensive-Portfolio/blob/main/L04__Grisel_Barrera_ITAI2373_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üìö ITAI 2373 Module 04: Text Representation Homework Lab\n",
        "## From Words to Numbers:\n",
        "### Student Name: (Grisel Barrera )\n",
        "\n",
        "### üéØ **Welcome to Your Text Representation Adventure!**\n",
        "\n",
        "You'll discover how computers transform human language into mathematical representations that machines can understand and process. This journey will take you from basic word counting to sophisticated embedding techniques used in modern AI systems.\n",
        "\n",
        "### üìÖ **5-Parts Learning Journey**\n",
        "- **Part 1-2**: Foundations & Sparse Representations (BOW, Preprocessing)\n",
        "- **Part 3**: TF-IDF & N-grams (Weighted Representations)\n",
        "- **Part 4**: Dense Representations (Word Embeddings)\n",
        "- **Part 5**: Integration & Real-World Applications\n",
        "\n",
        "### üéì **Learning Outcomes**\n",
        "By completing this lab, you will be able to:\n",
        "- Explain why text must be converted to numbers for machine learning\n",
        "- Implement Bag of Words and TF-IDF representations from scratch\n",
        "- Apply N-gram analysis to capture word sequences\n",
        "- Explore word embeddings and their semantic properties\n",
        "- Compare different text representation methods\n",
        "- Build a simple text classification system\n",
        "\n",
        "### üìã **Submission Guidelines**\n",
        "- Complete all exercises and answer all questions\n",
        "- Run all code cells and ensure outputs are visible\n",
        "- Provide thoughtful responses to reflection questions\n",
        "\n",
        "\n",
        "### üèÜ **Assessment Rubric**\n",
        "- **Technical Implementation (60%)**: Correct code, proper library usage, handling edge cases\n",
        "- **Conceptual Understanding (25%)**: Clear explanations, result interpretation\n",
        "- **Analysis & Reflection (15%)**: Critical thinking, real-world connections\n",
        "\n",
        "---\n",
        "**Let's begin your journey into the fascinating world of text representation!** üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## üîß Environment Setup\n",
        "\n",
        "First, let's install and import all the libraries we'll need for our text representation journey. Run the cells below to set up your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "import_libraries",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "7e9eb6c0-e913-48b7-d61c-64c1237818a7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-278119855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Gensim for word embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Set up plotting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Import all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import math\n",
        "from itertools import combinations\n",
        "\n",
        "# NLTK for text processing\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords, movie_reviews\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Scikit-learn for machine learning\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Gensim for word embeddings\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"üéâ You're ready to start your text representation journey!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "install_libraries",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aba83ea2-989f-4ede-c771-3218fe8916a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "scipy"
                ]
              },
              "id": "aa568a914b104ceda68de91c3d7c6915"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries (run this cell first in Google Colab)\n",
        "!pip install nltk gensim scikit-learn matplotlib seaborn wordcloud\n",
        "!python -m nltk.downloader punkt stopwords movie_reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "day1_header"
      },
      "source": [
        "# üìÖ Part 1-2: Foundations & Sparse Representations\n",
        "\n",
        "## ü§î Why Do We Need to Convert Text to Numbers?\n",
        "\n",
        "Imagine you're trying to teach a computer to understand the difference between \"I love this movie!\" and \"This movie is terrible.\" How would you explain the concept of sentiment to a machine that only understands mathematics?\n",
        "\n",
        "This is the fundamental challenge in Natural Language Processing (NLP). Computers are excellent at processing numbers, but human language is complex, contextual, and inherently non-numerical. We need a bridge between words and numbers.\n",
        "\n",
        "### üéØ **Part 1-2 Goals:**\n",
        "- Understand why text-to-number conversion is necessary\n",
        "- Master text preprocessing and tokenization\n",
        "- Implement Bag of Words (BOW) from scratch\n",
        "- Explore the limitations of sparse representations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sample_data"
      },
      "source": [
        "## üìù Our Sample Dataset\n",
        "\n",
        "Let's start with a small collection of movie reviews to make our learning concrete and relatable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "create_sample_data",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc590db0-b47b-4bdd-b0b1-1eb543113aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Sample Movie Reviews:\n",
            "\n",
            "1. [üòä Positive] This movie is absolutely fantastic! The acting is superb and the plot is engaging.\n",
            "\n",
            "2. [üòû Negative] I found this film quite boring. The story dragged on and the characters were flat.\n",
            "\n",
            "3. [üòä Positive] Amazing cinematography and brilliant performances. A must-watch movie!\n",
            "\n",
            "4. [üòû Negative] The plot was confusing and the dialogue felt forced. Not recommended.\n",
            "\n",
            "5. [üòä Positive] Great movie with excellent acting. The story kept me engaged throughout.\n",
            "\n",
            "üìä Dataset Summary: 5 reviews (3 positive, 2 negative)\n"
          ]
        }
      ],
      "source": [
        "# Our sample movie reviews for learning\n",
        "sample_reviews = [\n",
        "    \"This movie is absolutely fantastic! The acting is superb and the plot is engaging.\",\n",
        "    \"I found this film quite boring. The story dragged on and the characters were flat.\",\n",
        "    \"Amazing cinematography and brilliant performances. A must-watch movie!\",\n",
        "    \"The plot was confusing and the dialogue felt forced. Not recommended.\",\n",
        "    \"Great movie with excellent acting. The story kept me engaged throughout.\"\n",
        "]\n",
        "\n",
        "# Let's also create labels for sentiment (positive=1, negative=0)\n",
        "sample_labels = [1, 0, 1, 0, 1]  # 1 = positive, 0 = negative\n",
        "\n",
        "print(\"üìö Sample Movie Reviews:\")\n",
        "for i, (review, label) in enumerate(zip(sample_reviews, sample_labels)):\n",
        "    sentiment = \"üòä Positive\" if label == 1 else \"üòû Negative\"\n",
        "    print(f\"\\n{i+1}. [{sentiment}] {review}\")\n",
        "\n",
        "print(f\"\\nüìä Dataset Summary: {len(sample_reviews)} reviews ({sum(sample_labels)} positive, {len(sample_labels)-sum(sample_labels)} negative)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocessing_section"
      },
      "source": [
        "## üßπ Text Preprocessing: Cleaning Our Data\n",
        "\n",
        "Before we can convert text to numbers, we need to clean and standardize our text. Think of this as preparing ingredients before cooking - we need everything in the right format!\n",
        "\n",
        "### Common Preprocessing Steps:\n",
        "1. **Lowercasing**: \"Movie\" and \"movie\" should be treated the same\n",
        "2. **Removing punctuation**: \"great!\" becomes \"great\"\n",
        "3. **Tokenization**: Breaking text into individual words\n",
        "4. **Removing stop words**: Common words like \"the\", \"and\", \"is\"\n",
        "5. **Stemming**: \"running\", \"runs\", \"ran\" ‚Üí \"run\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "preprocessing_demo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "outputId": "c138c16b-1900-4aa5-d462-09b1f1af1a6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî§ Original text: This movie is absolutely fantastic! The acting is superb and the plot is engaging.\n",
            "\n",
            "1Ô∏è‚É£ After lowercasing: this movie is absolutely fantastic! the acting is superb and the plot is engaging.\n",
            "2Ô∏è‚É£ After removing punctuation: this movie is absolutely fantastic the acting is superb and the plot is engaging\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3504309714.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Step 3: Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"3Ô∏è‚É£ After tokenization: {tokens}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Let's see preprocessing in action with one example\n",
        "example_text = sample_reviews[0]\n",
        "print(f\"üî§ Original text: {example_text}\")\n",
        "\n",
        "# Step 1: Lowercase\n",
        "step1 = example_text.lower()\n",
        "print(f\"\\n1Ô∏è‚É£ After lowercasing: {step1}\")\n",
        "\n",
        "# Step 2: Remove punctuation\n",
        "step2 = re.sub(r'[^\\w\\s]', '', step1)\n",
        "print(f\"2Ô∏è‚É£ After removing punctuation: {step2}\")\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokens = word_tokenize(step2)\n",
        "print(f\"3Ô∏è‚É£ After tokenization: {tokens}\")\n",
        "\n",
        "# Step 4: Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(f\"4Ô∏è‚É£ After removing stop words: {filtered_tokens}\")\n",
        "\n",
        "# Step 5: Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(f\"5Ô∏è‚É£ After stemming: {stemmed_tokens}\")\n",
        "\n",
        "print(f\"\\nüìè Length reduction: {len(example_text.split())} ‚Üí {len(stemmed_tokens)} words\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise1"
      },
      "source": [
        "### üèãÔ∏è‚Äç‚ôÄÔ∏è **Exercise 1: Build Your Own Preprocessor**\n",
        "\n",
        "Now it's your turn! Complete the function below to preprocess text. This will be your foundation for all future exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "exercise1_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e3e15326-79f3-4658-df4c-d3bc32de32ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-7-1660947644.py, line 22)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-7-1660947644.py\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    text =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text, remove_stopwords=True, apply_stemming=True):\n",
        "    \"\"\"\n",
        "    Preprocess a text string by cleaning and tokenizing it.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to preprocess\n",
        "        remove_stopwords (bool): Whether to remove stop words\n",
        "        apply_stemming (bool): Whether to apply stemming\n",
        "\n",
        "    Returns:\n",
        "        list: List of preprocessed tokens\n",
        "    \"\"\"\n",
        "    # TODO: Implement the preprocessing steps\n",
        "    # Hint: Follow the same steps we demonstrated above\n",
        "\n",
        "    # Step 1: Convert to lowercase\n",
        "    text = step1 = example_text.lower()\n",
        "    print(f\"\\n1 After lowercasing: {step1}\")\n",
        "\n",
        "\n",
        "    # Step 2: Remove punctuation (keep only letters, numbers, and spaces)\n",
        "    text =  re.sub(r'[^\\w\\s]', '', step1)\n",
        "    print(f\"2 After removing punctuation: {step2}\")\n",
        "\n",
        "    # Step 3: Tokenize\n",
        "    tokens = word_tokenize(step2)\n",
        "    print(f\"3 After tokenization: {tokens}\")\n",
        "\n",
        "    # Step 4: Remove stop words (if requested)\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Step 5: Apply stemming (if requested)\n",
        "    if apply_stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test your function\n",
        "test_text = \"The movies are absolutely AMAZING! I love watching them.\"\n",
        "result = preprocess_text(test_text)\n",
        "print(f\"Input: {test_text}\")\n",
        "print(f\"Output: {result}\")\n",
        "\n",
        "# Expected output should be something like: ['movi', 'absolut', 'amaz', 'love', 'watch']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution1"
      },
      "source": [
        "**üí° Solution Check:** Run the cell below to see the expected solution and compare with your implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "solution1_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "a79c8900-19eb-4796-fa29-709c40d09415"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-1186648834.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Test the solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtest_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected output: {test_result}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n‚úÖ If your output matches this, great job! If not, review the steps above.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_text' is not defined"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 1\n",
        "def preprocess_text_solution(text, remove_stopwords=True, apply_stemming=True):\n",
        "    # Step 1: Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Step 2: Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Step 3: Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Step 4: Remove stop words\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Step 5: Apply stemming\n",
        "    if apply_stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Test the solution\n",
        "test_result = preprocess_text_solution(test_text)\n",
        "print(f\"Expected output: {test_result}\")\n",
        "print(\"\\n‚úÖ If your output matches this, great job! If not, review the steps above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "preprocess_all"
      },
      "source": [
        "Now let's preprocess all our sample reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "preprocess_all_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "outputId": "2291201e-e03c-4655-86c1-bf949dc8e948"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-2420771435.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Preprocess all sample reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocessed_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìù Preprocessed Reviews:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-10-2420771435.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Preprocess all sample reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreprocessed_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocess_text_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìù Preprocessed Reviews:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-8-1186648834.py\u001b[0m in \u001b[0;36mpreprocess_text_solution\u001b[0;34m(text, remove_stopwords, apply_stemming)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Step 3: Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Step 4: Remove stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "# Preprocess all sample reviews\n",
        "preprocessed_reviews = [preprocess_text_solution(review) for review in sample_reviews]\n",
        "\n",
        "print(\"üìù Preprocessed Reviews:\")\n",
        "for i, (original, processed) in enumerate(zip(sample_reviews, preprocessed_reviews)):\n",
        "    print(f\"\\n{i+1}. Original: {original[:50]}...\")\n",
        "    print(f\"   Processed: {processed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_section"
      },
      "source": [
        "## üéí Bag of Words (BOW): Your First Text Representation\n",
        "\n",
        "Imagine you have a bag and you throw all the words from a document into it. You lose the order of words, but you can count how many times each word appears. That's exactly what Bag of Words does!\n",
        "\n",
        "### üîç **How BOW Works:**\n",
        "1. Create a vocabulary of all unique words across all documents\n",
        "2. For each document, count how many times each word appears\n",
        "3. Represent each document as a vector of word counts\n",
        "\n",
        "### üìä **Example:**\n",
        "- Document 1: \"I love movies\"\n",
        "- Document 2: \"Movies are great\"\n",
        "- Vocabulary: [\"I\", \"love\", \"movies\", \"are\", \"great\"]\n",
        "- Doc 1 vector: [1, 1, 1, 0, 0]\n",
        "- Doc 2 vector: [0, 0, 1, 1, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bow_demo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "943862c5-f14e-4077-945a-8c4eaf804987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Simple Documents:\n",
            "Doc 1: ['love', 'movie']\n",
            "Doc 2: ['movie', 'great']\n",
            "Doc 3: ['love', 'great', 'film']\n",
            "\n",
            "üìñ Vocabulary: ['film', 'great', 'love', 'movie']\n",
            "\n",
            "üéí BOW Vectors:\n",
            "Doc 1: [0, 0, 1, 1]\n",
            "Doc 2: [0, 1, 0, 1]\n",
            "Doc 3: [1, 1, 1, 0]\n",
            "\n",
            "üìä BOW Matrix:\n",
            "       film  great  love  movie\n",
            "Doc 1     0      0     1      1\n",
            "Doc 2     0      1     0      1\n",
            "Doc 3     1      1     1      0\n"
          ]
        }
      ],
      "source": [
        "# Let's build BOW step by step with a simple example\n",
        "simple_docs = [\n",
        "    [\"love\", \"movie\"],\n",
        "    [\"movie\", \"great\"],\n",
        "    [\"love\", \"great\", \"film\"]\n",
        "]\n",
        "\n",
        "print(\"üìö Simple Documents:\")\n",
        "for i, doc in enumerate(simple_docs):\n",
        "    print(f\"Doc {i+1}: {doc}\")\n",
        "\n",
        "# Step 1: Build vocabulary\n",
        "vocabulary = sorted(set(word for doc in simple_docs for word in doc))\n",
        "print(f\"\\nüìñ Vocabulary: {vocabulary}\")\n",
        "\n",
        "# Step 2: Create BOW vectors\n",
        "bow_vectors = []\n",
        "for doc in simple_docs:\n",
        "    vector = [doc.count(word) for word in vocabulary]\n",
        "    bow_vectors.append(vector)\n",
        "\n",
        "print(f\"\\nüéí BOW Vectors:\")\n",
        "for i, vector in enumerate(bow_vectors):\n",
        "    print(f\"Doc {i+1}: {vector}\")\n",
        "\n",
        "# Visualize as a matrix\n",
        "bow_df = pd.DataFrame(bow_vectors, columns=vocabulary, index=[f\"Doc {i+1}\" for i in range(len(simple_docs))])\n",
        "print(f\"\\nüìä BOW Matrix:\")\n",
        "print(bow_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise2"
      },
      "source": [
        "### üèãÔ∏è‚Äç‚ôÄÔ∏è **Exercise 2: Build BOW from Scratch**\n",
        "\n",
        "Now implement your own BOW function! This will help you understand exactly how the representation works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "exercise2_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "297d2dd8-6328-40ec-a2a6-f3789988e1e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-13-1607623195.py, line 14)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-13-1607623195.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    vocabulary = # YOUR CODE HERE\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def build_bow_representation(documents):\n",
        "    \"\"\"\n",
        "    Build Bag of Words representation for a list of documents.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents, where each document is a list of tokens\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocabulary, bow_matrix)\n",
        "            vocabulary (list): Sorted list of unique words\n",
        "            bow_matrix (list): List of BOW vectors for each document\n",
        "    \"\"\"\n",
        "    # TODO: Build the vocabulary (unique words across all documents)\n",
        "    vocabulary = #\n",
        "\n",
        "    # TODO: Create BOW vectors for each document\n",
        "    bow_matrix = []\n",
        "    for doc in documents:\n",
        "        # Create a vector where each element is the count of the corresponding vocabulary word\n",
        "        vector = # YOUR CODE HERE\n",
        "        bow_matrix.append(vector)\n",
        "\n",
        "    return vocabulary, bow_matrix\n",
        "\n",
        "# Test your function with our preprocessed reviews\n",
        "vocab, bow_matrix = build_bow_representation(preprocessed_reviews)\n",
        "\n",
        "print(f\"üìñ Vocabulary size: {len(vocab)}\")\n",
        "print(f\"üìñ First 10 words: {vocab[:10]}\")\n",
        "print(f\"\\nüéí BOW matrix shape: {len(bow_matrix)} documents √ó {len(vocab)} words\")\n",
        "print(f\"üéí First document vector (first 10 elements): {bow_matrix[0][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution2"
      },
      "source": [
        "**üí° Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "solution2_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "cc72b0bb-ca42-474e-8924-5bda3dc5c321"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocessed_reviews' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3491177342.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Test the solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mvocab_sol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbow_matrix_sol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_bow_representation_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Solution vocabulary size: {len(vocab_sol)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"‚úÖ Solution BOW matrix shape: {len(bow_matrix_sol)} √ó {len(vocab_sol)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_reviews' is not defined"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 2\n",
        "def build_bow_representation_solution(documents):\n",
        "    # Build vocabulary: get all unique words and sort them\n",
        "    vocabulary = sorted(set(word for doc in documents for word in doc))\n",
        "\n",
        "    # Create BOW vectors\n",
        "    bow_matrix = []\n",
        "    for doc in documents:\n",
        "        vector = [doc.count(word) for word in vocabulary]\n",
        "        bow_matrix.append(vector)\n",
        "\n",
        "    return vocabulary, bow_matrix\n",
        "\n",
        "# Test the solution\n",
        "vocab_sol, bow_matrix_sol = build_bow_representation_solution(preprocessed_reviews)\n",
        "print(f\"‚úÖ Solution vocabulary size: {len(vocab_sol)}\")\n",
        "print(f\"‚úÖ Solution BOW matrix shape: {len(bow_matrix_sol)} √ó {len(vocab_sol)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_sklearn"
      },
      "source": [
        "### üî¨ Comparing with Scikit-learn's CountVectorizer\n",
        "\n",
        "Let's see how our implementation compares with the professional library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bow_sklearn_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8a16574c-410f-4cf9-d057-4ac446073726"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CountVectorizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-67570356.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Using scikit-learn's CountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# We need to join our preprocessed tokens back into strings for sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocessed_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_reviews\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Using scikit-learn's CountVectorizer\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "\n",
        "# We need to join our preprocessed tokens back into strings for sklearn\n",
        "processed_texts = [' '.join(tokens) for tokens in preprocessed_reviews]\n",
        "sklearn_bow = vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "print(\"üî¨ Scikit-learn CountVectorizer Results:\")\n",
        "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
        "print(f\"BOW matrix shape: {sklearn_bow.shape}\")\n",
        "print(f\"Matrix type: {type(sklearn_bow)}\")\n",
        "\n",
        "# Convert to dense array for comparison\n",
        "sklearn_bow_dense = sklearn_bow.toarray()\n",
        "print(f\"\\nüìä First document vector (first 10 elements): {sklearn_bow_dense[0][:10]}\")\n",
        "\n",
        "# Show some vocabulary words\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(f\"\\nüìñ First 10 vocabulary words: {feature_names[:10].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_visualization"
      },
      "source": [
        "### üìä Visualizing BOW Representations\n",
        "\n",
        "Let's create some visualizations to better understand our BOW representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bow_viz_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b0469ddd-cc81-4912-f3a5-51e398e602f8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-199533803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a DataFrame for better visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m bow_df = pd.DataFrame(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0msklearn_bow_dense\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"Review {i+1}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Create a DataFrame for better visualization\n",
        "bow_df = pd.DataFrame(\n",
        "    sklearn_bow_dense,\n",
        "    columns=feature_names,\n",
        "    index=[f\"Review {i+1}\" for i in range(len(sample_reviews))]\n",
        ")\n",
        "\n",
        "# 1. Heatmap of BOW representation\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Show only words that appear at least once\n",
        "active_words = bow_df.columns[bow_df.sum() > 0][:20]  # Top 20 most frequent words\n",
        "sns.heatmap(bow_df[active_words], annot=True, cmap='Blues', fmt='d')\n",
        "plt.title('üéí Bag of Words Heatmap (Top 20 Words)')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Reviews')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Word frequency distribution\n",
        "word_frequencies = bow_df.sum().sort_values(ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "word_frequencies[:15].plot(kind='bar')\n",
        "plt.title('üìä Top 15 Most Frequent Words')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"üìà Total unique words: {len(feature_names)}\")\n",
        "print(f\"üìà Average words per review: {bow_df.sum(axis=1).mean():.1f}\")\n",
        "print(f\"üìà Sparsity: {(bow_df == 0).sum().sum() / (bow_df.shape[0] * bow_df.shape[1]) * 100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_limitations"
      },
      "source": [
        "## üö® BOW Limitations: What Are We Missing?\n",
        "\n",
        "BOW is simple and effective, but it has some important limitations. Let's explore them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bow_limitations_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "8ba76c2c-3c00-4ad5-aac3-97536641bb2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üö® BOW Limitation Examples:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocess_text_solution' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2431349101.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üö® BOW Limitation Examples:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimitation_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_text_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_stemming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{i+1}. Text: '{text}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"   Tokens: {tokens}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocess_text_solution' is not defined"
          ]
        }
      ],
      "source": [
        "# Demonstrating BOW limitations\n",
        "limitation_examples = [\n",
        "    \"The dog ate my homework\",\n",
        "    \"The homework ate my dog\",  # Same words, different meaning!\n",
        "    \"This movie is not bad\",\n",
        "    \"This movie is bad\"  # Negation lost!\n",
        "]\n",
        "\n",
        "print(\"üö® BOW Limitation Examples:\")\n",
        "for i, text in enumerate(limitation_examples):\n",
        "    tokens = preprocess_text_solution(text, remove_stopwords=False, apply_stemming=False)\n",
        "    print(f\"\\n{i+1}. Text: '{text}'\")\n",
        "    print(f\"   Tokens: {tokens}\")\n",
        "\n",
        "# Show that different sentences can have identical BOW representations\n",
        "vectorizer_demo = CountVectorizer(lowercase=True)\n",
        "bow_demo = vectorizer_demo.fit_transform(limitation_examples)\n",
        "\n",
        "print(\"\\nüìä BOW Vectors:\")\n",
        "feature_names_demo = vectorizer_demo.get_feature_names_out()\n",
        "for i, vector in enumerate(bow_demo.toarray()):\n",
        "    print(f\"Text {i+1}: {vector}\")\n",
        "\n",
        "# Check if any vectors are identical\n",
        "if np.array_equal(bow_demo.toarray()[0], bow_demo.toarray()[1]):\n",
        "    print(\"\\n‚ö†Ô∏è Texts 1 and 2 have IDENTICAL BOW representations despite different meanings!\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ Texts 1 and 2 have different BOW representations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection1"
      },
      "source": [
        "### ü§î **Reflection Questions - Part 1-2**\n",
        "\n",
        "Answer these questions to consolidate your understanding:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection1_questions"
      },
      "source": [
        "**Question 1:** Why can't machine learning algorithms work directly with text? Explain in your own words.\n",
        "\n",
        "**Your Answer:**\n",
        "It can't just read text the way we do. It doesnt understand the way we do. Text is letters and characters, and computers only deal with numbers. Before any algorithm can work with text, we have to convert it into numbers. We have to turn words into vectors or tokens- so the machine can process it, spot patterns and learn from it.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2:** What information is lost when we use Bag of Words representation? Give a specific example.\n",
        "\n",
        "**Your Answer:**\n",
        "We lose the order of the words (BoW) model, and how they relate to each other. It's like dumping all the words in one bag and just counting how many times each word shows up, not paying attention to where they we're in the sentence. That means the meaning or context can get messed up.  \n",
        "\n",
        "---\n",
        "\n",
        "**Question 3:** Look at the sparsity percentage from our BOW visualization above. What does this tell us about the efficiency of BOW representation?\n",
        "\n",
        "**Your Answer:**\n",
        "The high sparcity in our (BoW) model shows that most of our values in our document is just zeros. That means for each sentence or document, only a few words from the whole vocabularyare actually used. The rest of them are just taking up space with zeros. So it's not very efficient, especially when the vocabulary gets huge. We're wasting a lot of memory and processing power on words that are not even there.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4:** In what scenarios might BOW representation still be useful despite its limitations?\n",
        "\n",
        "**Your Answer:**\n",
        "Even though (BoW) has its flaws, like losing word order and being super sparse, it can still be useful in certain situations-especially if the text is short, the vocabulary is small, or you don't need deep meaning. Like for spam detection, BoW can work great because certain words like \"free\", \"winner\", or \"click\" show up alot in spam emails. Its fast and doesn't need alot of computing power, so it's good when working with limited resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "day3_header"
      },
      "source": [
        "# üìÖ Part 3: TF-IDF & N-grams - Weighted Representations\n",
        "\n",
        "## üéØ **Part 3 Goals:**\n",
        "- Understand and implement TF-IDF weighting\n",
        "- Explore N-gram analysis for capturing word sequences\n",
        "- Calculate document similarity using cosine similarity\n",
        "- Compare different representation methods\n",
        "\n",
        "## ‚öñÔ∏è TF-IDF: Not All Words Are Created Equal\n",
        "\n",
        "Imagine you're reading movie reviews. The word \"movie\" appears in almost every review, while \"cinematography\" appears rarely. Which word tells you more about a specific review?\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) solves this by giving higher weights to words that are:\n",
        "- **Frequent in the document** (TF - Term Frequency)\n",
        "- **Rare across the collection** (IDF - Inverse Document Frequency)\n",
        "\n",
        "### üìê **Mathematical Foundation:**\n",
        "- **TF(term, doc)** = count(term) / total_terms_in_doc\n",
        "- **IDF(term)** = log(N_docs / (N_docs_containing_term + 1))\n",
        "- **TF-IDF** = TF √ó IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_manual"
      },
      "source": [
        "### üßÆ Manual TF-IDF Calculation\n",
        "\n",
        "Let's calculate TF-IDF step by step to understand the math:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tfidf_manual_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 679
        },
        "outputId": "03ceb68a-bf88-4407-b640-8edd49772041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Simple Corpus for TF-IDF Calculation:\n",
            "Doc 1: 'the movie is great'\n",
            "Doc 2: 'the film is excellent'\n",
            "\n",
            "üî§ Tokenized: [['the', 'movie', 'is', 'great'], ['the', 'film', 'is', 'excellent']]\n",
            "\n",
            "üìñ Vocabulary: ['excellent', 'film', 'great', 'is', 'movie', 'the']\n",
            "\n",
            "üìä Term Frequency (TF) Calculation:\n",
            "\n",
            "Doc 1 (length: 4):\n",
            "  'excellent': count=0, TF=0.000\n",
            "  'film': count=0, TF=0.000\n",
            "  'great': count=1, TF=0.250\n",
            "  'is': count=1, TF=0.250\n",
            "  'movie': count=1, TF=0.250\n",
            "  'the': count=1, TF=0.250\n",
            "\n",
            "Doc 2 (length: 4):\n",
            "  'excellent': count=1, TF=0.250\n",
            "  'film': count=1, TF=0.250\n",
            "  'great': count=0, TF=0.000\n",
            "  'is': count=1, TF=0.250\n",
            "  'movie': count=0, TF=0.000\n",
            "  'the': count=1, TF=0.250\n",
            "\n",
            "üìä Inverse Document Frequency (IDF) Calculation:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'math' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-102680428.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mdocs_containing_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_docs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_docs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdocs_containing_word\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0midf_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  '{word}': appears in {docs_containing_word}/{n_docs} docs, IDF={idf:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'math' is not defined"
          ]
        }
      ],
      "source": [
        "# Simple example for manual TF-IDF calculation\n",
        "simple_corpus = [\n",
        "    \"the movie is great\",\n",
        "    \"the film is excellent\"\n",
        "]\n",
        "\n",
        "print(\"üìö Simple Corpus for TF-IDF Calculation:\")\n",
        "for i, doc in enumerate(simple_corpus):\n",
        "    print(f\"Doc {i+1}: '{doc}'\")\n",
        "\n",
        "# Tokenize documents\n",
        "tokenized_docs = [doc.split() for doc in simple_corpus]\n",
        "print(f\"\\nüî§ Tokenized: {tokenized_docs}\")\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "print(f\"\\nüìñ Vocabulary: {vocab}\")\n",
        "\n",
        "# Calculate TF for each document\n",
        "print(\"\\nüìä Term Frequency (TF) Calculation:\")\n",
        "tf_matrix = []\n",
        "for i, doc in enumerate(tokenized_docs):\n",
        "    doc_length = len(doc)\n",
        "    tf_vector = []\n",
        "    print(f\"\\nDoc {i+1} (length: {doc_length}):\")\n",
        "    for word in vocab:\n",
        "        count = doc.count(word)\n",
        "        tf = count / doc_length\n",
        "        tf_vector.append(tf)\n",
        "        print(f\"  '{word}': count={count}, TF={tf:.3f}\")\n",
        "    tf_matrix.append(tf_vector)\n",
        "\n",
        "# Calculate IDF\n",
        "print(\"\\nüìä Inverse Document Frequency (IDF) Calculation:\")\n",
        "n_docs = len(tokenized_docs)\n",
        "idf_vector = []\n",
        "for word in vocab:\n",
        "    docs_containing_word = sum(1 for doc in tokenized_docs if word in doc)\n",
        "    idf = math.log(n_docs / (docs_containing_word + 1))\n",
        "    idf_vector.append(idf)\n",
        "    print(f\"  '{word}': appears in {docs_containing_word}/{n_docs} docs, IDF={idf:.3f}\")\n",
        "\n",
        "# Calculate TF-IDF\n",
        "print(\"\\nüìä TF-IDF Calculation:\")\n",
        "tfidf_matrix = []\n",
        "for i, tf_vector in enumerate(tf_matrix):\n",
        "    tfidf_vector = [tf * idf for tf, idf in zip(tf_vector, idf_vector)]\n",
        "    tfidf_matrix.append(tfidf_vector)\n",
        "    print(f\"\\nDoc {i+1} TF-IDF:\")\n",
        "    for j, (word, tfidf) in enumerate(zip(vocab, tfidf_vector)):\n",
        "        print(f\"  '{word}': {tfidf:.3f}\")\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix, columns=vocab, index=[f\"Doc {i+1}\" for i in range(len(simple_corpus))])\n",
        "print(\"\\nüìä TF-IDF Matrix:\")\n",
        "print(tfidf_df.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise3"
      },
      "source": [
        "### üèãÔ∏è‚Äç‚ôÄÔ∏è **Exercise 3: Implement TF-IDF from Scratch**\n",
        "\n",
        "Now implement your own TF-IDF function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "exercise3_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "613bdc60-5397-4526-a974-0a6253bffc5f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-37-3906877382.py, line 19)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-37-3906877382.py\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    docs_containing_word =\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def calculate_tfidf(documents):\n",
        "    \"\"\"\n",
        "    Calculate TF-IDF representation for a list of documents.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents, where each document is a list of tokens\n",
        "\n",
        "    Returns:\n",
        "        tuple: (vocabulary, tfidf_matrix)\n",
        "    \"\"\"\n",
        "    # Build vocabulary\n",
        "    vocabulary = sorted(set(word for doc in documents for word in doc))\n",
        "    n_docs = len(documents)\n",
        "\n",
        "    # Calculate IDF for each word\n",
        "    idf_vector = []\n",
        "    for word in vocabulary:\n",
        "        # TODO: Count how many documents contain this word\n",
        "        docs_containing_word =\n",
        "\n",
        "        # TODO: Calculate IDF using the formula: log(n_docs / (docs_containing_word + 1))\n",
        "        idf =\n",
        "        idf_vector.append(idf)\n",
        "\n",
        "    # Calculate TF-IDF for each document\n",
        "    tfidf_matrix = []\n",
        "    for doc in documents:\n",
        "        doc_length = len(doc)\n",
        "        tfidf_vector = []\n",
        "\n",
        "        for i, word in enumerate(vocabulary):\n",
        "            # TODO: Calculate TF (term frequency)\n",
        "            tf =\n",
        "\n",
        "            # TODO: Calculate TF-IDF by multiplying TF and IDF\n",
        "            tfidf =\n",
        "            tfidf_vector.append(tfidf)\n",
        "\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "\n",
        "    return vocabulary, tfidf_matrix\n",
        "\n",
        "# Test your function\n",
        "test_docs = [[\"movie\", \"great\"], [\"film\", \"excellent\"], [\"movie\", \"excellent\"]]\n",
        "vocab, tfidf_result = calculate_tfidf(test_docs)\n",
        "\n",
        "print(f\"Vocabulary: {vocab}\")\n",
        "print(f\"TF-IDF Matrix:\")\n",
        "for i, vector in enumerate(tfidf_result):\n",
        "    print(f\"Doc {i+1}: {[round(x, 3) for x in vector]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution3"
      },
      "source": [
        "**üí° Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "solution3_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "410bb2f0-97fb-433a-a404-5f40a4436900"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_docs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-3639787753.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Test solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mvocab_sol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_sol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_tfidf_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Solution TF-IDF Matrix:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_sol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_docs' is not defined"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 3\n",
        "def calculate_tfidf_solution(documents):\n",
        "    vocabulary = sorted(set(word for doc in documents for word in doc))\n",
        "    n_docs = len(documents)\n",
        "\n",
        "    # Calculate IDF\n",
        "    idf_vector = []\n",
        "    for word in vocabulary:\n",
        "        docs_containing_word = sum(1 for doc in documents if word in doc)\n",
        "        idf = math.log(n_docs / (docs_containing_word + 1))\n",
        "        idf_vector.append(idf)\n",
        "\n",
        "    # Calculate TF-IDF\n",
        "    tfidf_matrix = []\n",
        "    for doc in documents:\n",
        "        doc_length = len(doc)\n",
        "        tfidf_vector = []\n",
        "\n",
        "        for i, word in enumerate(vocabulary):\n",
        "            tf = doc.count(word) / doc_length\n",
        "            tfidf = tf * idf_vector[i]\n",
        "            tfidf_vector.append(tfidf)\n",
        "\n",
        "        tfidf_matrix.append(tfidf_vector)\n",
        "\n",
        "    return vocabulary, tfidf_matrix\n",
        "\n",
        "# Test solution\n",
        "vocab_sol, tfidf_sol = calculate_tfidf_solution(test_docs)\n",
        "print(\"‚úÖ Solution TF-IDF Matrix:\")\n",
        "for i, vector in enumerate(tfidf_sol):\n",
        "    print(f\"Doc {i+1}: {[round(x, 3) for x in vector]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfidf_sklearn"
      },
      "source": [
        "### üî¨ Comparing with Scikit-learn's TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tfidf_sklearn_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "37744f63-2940-4a1c-f539-fa092600cfa8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TfidfVectorizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-4178705439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply TF-IDF to our movie reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtfidf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üî¨ Scikit-learn TfidfVectorizer Results:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Apply TF-IDF to our movie reviews\n",
        "tfidf_vectorizer = TfidfVectorizer(lowercase=True, stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_texts)\n",
        "\n",
        "print(\"üî¨ Scikit-learn TfidfVectorizer Results:\")\n",
        "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
        "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# Get feature names and convert to dense array\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_dense = tfidf_matrix.toarray()\n",
        "\n",
        "# Create DataFrame for visualization\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_dense,\n",
        "    columns=feature_names,\n",
        "    index=[f\"Review {i+1}\" for i in range(len(sample_reviews))]\n",
        ")\n",
        "\n",
        "# Show top TF-IDF words for each document\n",
        "print(\"\\nüèÜ Top 5 TF-IDF words for each review:\")\n",
        "for i, review_idx in enumerate(tfidf_df.index):\n",
        "    top_words = tfidf_df.loc[review_idx].nlargest(5)\n",
        "    print(f\"\\n{review_idx}:\")\n",
        "    for word, score in top_words.items():\n",
        "        if score > 0:\n",
        "            print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "# Visualize TF-IDF heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Show only words with non-zero TF-IDF scores\n",
        "active_words = tfidf_df.columns[tfidf_df.sum() > 0][:20]\n",
        "sns.heatmap(tfidf_df[active_words], annot=True, cmap='Reds', fmt='.2f')\n",
        "plt.title('üî• TF-IDF Heatmap (Top 20 Words)')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Reviews')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngrams_section"
      },
      "source": [
        "## üîó N-grams: Capturing Word Sequences\n",
        "\n",
        "Remember how BOW lost word order? N-grams help us capture some of that information by looking at sequences of words:\n",
        "\n",
        "- **Unigrams (1-gram)**: Individual words [\"great\", \"movie\"]\n",
        "- **Bigrams (2-gram)**: Word pairs [\"great movie\", \"movie is\"]\n",
        "- **Trigrams (3-gram)**: Word triplets [\"great movie is\", \"movie is amazing\"]\n",
        "\n",
        "### üéØ **Why N-grams Matter:**\n",
        "- \"not good\" vs \"good\" - bigrams capture negation\n",
        "- \"New York\" - should be treated as one entity\n",
        "- \"very good\" vs \"good\" - intensity matters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ngrams_demo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7100e5a-2e69-44cc-8a35-ce9be6591263"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìù Example text: 'This movie is not very good at all'\n",
            "üî§ Tokens: ['this', 'movie', 'is', 'not', 'very', 'good', 'at', 'all']\n",
            "\n",
            "1-grams: ['this', 'movie', 'is', 'not', 'very', 'good', 'at', 'all']\n",
            "\n",
            "2-grams: ['this movie', 'movie is', 'is not', 'not very', 'very good', 'good at', 'at all']\n",
            "\n",
            "3-grams: ['this movie is', 'movie is not', 'is not very', 'not very good', 'very good at', 'good at all']\n",
            "\n",
            "üîç Information Captured:\n",
            "‚Ä¢ Unigrams: Individual word importance\n",
            "‚Ä¢ Bigrams: 'not very', 'very good' - captures negation and intensity\n",
            "‚Ä¢ Trigrams: 'not very good' - captures complex sentiment patterns\n"
          ]
        }
      ],
      "source": [
        "def generate_ngrams(tokens, n):\n",
        "    \"\"\"\n",
        "    Generate n-grams from a list of tokens.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): List of tokens\n",
        "        n (int): Size of n-grams\n",
        "\n",
        "    Returns:\n",
        "        list: List of n-grams\n",
        "    \"\"\"\n",
        "    if len(tokens) < n:\n",
        "        return []\n",
        "\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = ' '.join(tokens[i:i+n])\n",
        "        ngrams.append(ngram)\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "# Demonstrate n-grams with an example\n",
        "example_text = \"This movie is not very good at all\"\n",
        "example_tokens = example_text.lower().split()\n",
        "\n",
        "print(f\"üìù Example text: '{example_text}'\")\n",
        "print(f\"üî§ Tokens: {example_tokens}\")\n",
        "\n",
        "# Generate different n-grams\n",
        "for n in range(1, 4):\n",
        "    ngrams = generate_ngrams(example_tokens, n)\n",
        "    print(f\"\\n{n}-grams: {ngrams}\")\n",
        "\n",
        "# Show how n-grams capture different information\n",
        "print(\"\\nüîç Information Captured:\")\n",
        "print(\"‚Ä¢ Unigrams: Individual word importance\")\n",
        "print(\"‚Ä¢ Bigrams: 'not very', 'very good' - captures negation and intensity\")\n",
        "print(\"‚Ä¢ Trigrams: 'not very good' - captures complex sentiment patterns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise4"
      },
      "source": [
        "### üèãÔ∏è‚Äç‚ôÄÔ∏è **Exercise 4: N-gram Analysis**\n",
        "\n",
        "Analyze the most common n-grams in our movie reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "exercise4_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3df14b84-682a-4ce0-83e7-060db932247d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-11-1530345445.py, line 17)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-11-1530345445.py\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    ngrams = # YOUR CODE HERE (use the generate_ngrams function)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def analyze_ngrams(documents, n, top_k=10):\n",
        "    \"\"\"\n",
        "    Analyze the most common n-grams across documents.\n",
        "\n",
        "    Args:\n",
        "        documents (list): List of documents (each is a list of tokens)\n",
        "        n (int): Size of n-grams\n",
        "        top_k (int): Number of top n-grams to return\n",
        "\n",
        "    Returns:\n",
        "        list: List of (ngram, frequency) tuples\n",
        "    \"\"\"\n",
        "    all_ngrams = []\n",
        "\n",
        "    # TODO: Generate n-grams for all documents\n",
        "    for doc in documents:\n",
        "        ngrams = generate_ngrams(doc, n)\n",
        "        all_ngrams.extend(ngrams)\n",
        "\n",
        "    # TODO: Count n-gram frequencies\n",
        "    ngram_counts = Counter(all_ngrams)\n",
        "\n",
        "    # TODO: Return top k most common n-grams\n",
        "    return ngram_counts.most_common(top_k)\n",
        "\n",
        "# Analyze n-grams in our preprocessed reviews\n",
        "print(\"üìä N-gram Analysis of Movie Reviews:\")\n",
        "\n",
        "for n in range(1, 4):\n",
        "    top_ngrams = analyze_ngrams(preprocessed_reviews, n, top_k=5)\n",
        "    print(f\"\\nüèÜ Top 5 {n}-grams:\")\n",
        "    for ngram, count in top_ngrams:\n",
        "        print(f\"  '{ngram}': {count}\")\n",
        "\n",
        "# Visualize bigram frequencies\n",
        "bigrams = analyze_ngrams(preprocessed_reviews, 2, top_k=10)\n",
        "if bigrams:\n",
        "    bigram_df = pd.DataFrame(bigrams, columns=['Bigram', 'Frequency'])\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(bigram_df['Bigram'], bigram_df['Frequency'])\n",
        "    plt.title('üîó Top 10 Bigrams in Movie Reviews')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel('Bigrams')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution4"
      },
      "source": [
        "**üí° Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "solution4_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "1d17e84b-a257-43b5-e796-9ce0d8824c56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Solution - Top 5 bigrams:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'preprocessed_reviews' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-3754195847.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Test solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Solution - Top 5 bigrams:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msolution_bigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_ngrams_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msolution_bigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  '{ngram}': {count}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_reviews' is not defined"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 4\n",
        "def analyze_ngrams_solution(documents, n, top_k=10):\n",
        "    all_ngrams = []\n",
        "\n",
        "    for doc in documents:\n",
        "        ngrams = generate_ngrams(doc, n)\n",
        "        all_ngrams.extend(ngrams)\n",
        "\n",
        "    ngram_counts = Counter(all_ngrams)\n",
        "    return ngram_counts.most_common(top_k)\n",
        "\n",
        "# Test solution\n",
        "print(\"‚úÖ Solution - Top 5 bigrams:\")\n",
        "solution_bigrams = analyze_ngrams_solution(preprocessed_reviews, 2, 5)\n",
        "for ngram, count in solution_bigrams:\n",
        "    print(f\"  '{ngram}': {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cosine_similarity"
      },
      "source": [
        "## üìê Document Similarity with Cosine Similarity\n",
        "\n",
        "Now that we have numerical representations, we can measure how similar documents are! Cosine similarity measures the angle between two vectors:\n",
        "\n",
        "**Formula:** sim(a,b) = (a¬∑b) / (||a|| ||b||) = cos(Œ±)\n",
        "\n",
        "- **1.0**: Identical documents (0¬∞ angle)\n",
        "- **0.0**: Completely different documents (90¬∞ angle)\n",
        "- **-1.0**: Opposite documents (180¬∞ angle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "cosine_demo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "84fe5e52-85ef-48c6-f453-49b1ce8d277c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cosine_similarity' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-18-3911082813.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate cosine similarity between our movie reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimilarity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üìê Cosine Similarity Matrix (TF-IDF):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m similarity_df = pd.DataFrame(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cosine_similarity' is not defined"
          ]
        }
      ],
      "source": [
        "# Calculate cosine similarity between our movie reviews\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"üìê Cosine Similarity Matrix (TF-IDF):\")\n",
        "similarity_df = pd.DataFrame(\n",
        "    similarity_matrix,\n",
        "    index=[f\"Review {i+1}\" for i in range(len(sample_reviews))],\n",
        "    columns=[f\"Review {i+1}\" for i in range(len(sample_reviews))]\n",
        ")\n",
        "print(similarity_df.round(3))\n",
        "\n",
        "# Visualize similarity matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(similarity_df, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, fmt='.3f')\n",
        "plt.title('üìê Document Similarity Heatmap (TF-IDF + Cosine Similarity)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find most similar document pairs\n",
        "print(\"\\nüîç Most Similar Document Pairs:\")\n",
        "for i in range(len(sample_reviews)):\n",
        "    for j in range(i+1, len(sample_reviews)):\n",
        "        similarity = similarity_matrix[i][j]\n",
        "        print(f\"Review {i+1} ‚Üî Review {j+1}: {similarity:.3f}\")\n",
        "        if similarity > 0.3:  # Threshold for \"similar\"\n",
        "            print(f\"  üìù Review {i+1}: {sample_reviews[i][:50]}...\")\n",
        "            print(f\"  üìù Review {j+1}: {sample_reviews[j][:50]}...\")\n",
        "            print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bow_vs_tfidf"
      },
      "source": [
        "### ‚öñÔ∏è BOW vs TF-IDF Comparison\n",
        "\n",
        "Let's compare how BOW and TF-IDF perform for document similarity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "comparison_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "0dbcb076-0b00-490d-d2f3-c52fa586aee9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'cosine_similarity' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-4206629256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Calculate BOW similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbow_similarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Compare BOW vs TF-IDF similarities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚öñÔ∏è BOW vs TF-IDF Similarity Comparison:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cosine_similarity' is not defined"
          ]
        }
      ],
      "source": [
        "# Calculate BOW similarity\n",
        "bow_similarity = cosine_similarity(sklearn_bow)\n",
        "\n",
        "# Compare BOW vs TF-IDF similarities\n",
        "print(\"‚öñÔ∏è BOW vs TF-IDF Similarity Comparison:\")\n",
        "print(\"\\nBOW Similarities:\")\n",
        "bow_sim_df = pd.DataFrame(\n",
        "    bow_similarity,\n",
        "    index=[f\"Review {i+1}\" for i in range(len(sample_reviews))],\n",
        "    columns=[f\"Review {i+1}\" for i in range(len(sample_reviews))]\n",
        ")\n",
        "print(bow_sim_df.round(3))\n",
        "\n",
        "print(\"\\nTF-IDF Similarities:\")\n",
        "print(similarity_df.round(3))\n",
        "\n",
        "# Visualize the comparison\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "sns.heatmap(bow_sim_df, annot=True, cmap='Blues', ax=ax1,\n",
        "            square=True, fmt='.3f', vmin=0, vmax=1)\n",
        "ax1.set_title('üéí BOW Similarity')\n",
        "\n",
        "sns.heatmap(similarity_df, annot=True, cmap='Reds', ax=ax2,\n",
        "            square=True, fmt='.3f', vmin=0, vmax=1)\n",
        "ax2.set_title('üî• TF-IDF Similarity')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate differences\n",
        "diff_matrix = similarity_matrix - bow_similarity\n",
        "print(f\"\\nüìä Average difference (TF-IDF - BOW): {np.mean(np.abs(diff_matrix)):.3f}\")\n",
        "print(f\"üìä Max difference: {np.max(np.abs(diff_matrix)):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection3"
      },
      "source": [
        "### ü§î **Reflection Questions - Part 3**\n",
        "\n",
        "**Question 1:** How does TF-IDF improve upon simple word counts? Explain with an example.\n",
        "\n",
        "**Your Answer:**\n",
        "It asks a smart question, for example: Is this word really imporatnt, or does it just show up everywhere? Imagine you have 100 documents, and: \"car\" appears in 1 document but \"the\" appears in all 100. In this scenario, TF-IDF lowers it's weight because it is in everything. But \"car\" appears once in 1 doc? It gets high score- because its specific and rare. It's way smarter. It tells you what is imprortant.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2:** What advantages do bigrams and trigrams provide over unigrams? Give specific examples from the n-gram analysis above.\n",
        "\n",
        "**Your Answer:**\n",
        "Unigram is 1 word at a time, Bigram is 2-word pairs, and tri gram is 3 word combos. Unigrams lose meaning. If I say: \"heat\" and \"map\" - are we talking about fire or gps? But if I use bigram: \"heatmap\" --> now it's clear.  Unigrams are pieces. Bigrams and trigrams give you the sentences flavor.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 3:** Looking at the similarity matrices, which method (BOW or TF-IDF) seems to provide more meaningful similarity scores? Why?\n",
        "\n",
        "**Your Answer:**\n",
        "TF-IDF gives more meaningful similarity scores than BoW. BoW just counts words. It treats every word equally. Two documents might look similar in BoW just because they both say \"the car is fast\", even if one is about cars and the other is about bikes. BoW is higher in score versus TF-IDF is lower. BoW is basic and surface level. TF-IDF is deeper - and looks at what really defines the document.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4:** What are the computational trade-offs of using higher-order n-grams (trigrams, 4-grams, etc.)?\n",
        "\n",
        "**Your Answer:**\n",
        "Higher-order n-grams (like trigrams or 4-grams) give more context than unigrams, but they come with trade offs. They dramatically increase the size of your feature space, making models slower and more memory-hungry. They also lead to very sparse data and can cause over fitting by capturing overly specific phrases. While they can be useful for short texts or phrase level analysis, they're less efficient for large scale tasks without smart filtering or dimensionality reduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "day4_header"
      },
      "source": [
        "# üìÖ Part 4: Dense Representations - Word Embeddings\n",
        "\n",
        "## üéØ **Part  4 Goals:**\n",
        "- Understand the distributional hypothesis\n",
        "- Explore pre-trained word embeddings (Word2Vec, GloVe)\n",
        "- Discover semantic relationships through word arithmetic\n",
        "- Compare sparse vs dense representations\n",
        "\n",
        "## üåü The Revolution: From Sparse to Dense\n",
        "\n",
        "So far, we've worked with **sparse representations** - vectors with mostly zeros. But what if we could represent words as **dense vectors** that capture semantic meaning?\n",
        "\n",
        "### üß† **The Distributional Hypothesis:**\n",
        "*\"You shall know a word by the company it keeps\"* - J.R. Firth (1957)\n",
        "\n",
        "Words that appear in similar contexts tend to have similar meanings:\n",
        "- \"The cat sat on the mat\" vs \"The dog sat on the mat\"\n",
        "- \"cat\" and \"dog\" appear in similar contexts ‚Üí they're semantically related\n",
        "\n",
        "### üéØ **Word Embeddings Benefits:**\n",
        "- **Dense**: 50-300 dimensions instead of 10,000+\n",
        "- **Semantic**: Similar words have similar vectors\n",
        "- **Arithmetic**: king - man + woman ‚âà queen\n",
        "- **Efficient**: Faster computation and storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_embeddings"
      },
      "source": [
        "## üì• Loading Pre-trained Word Embeddings\n",
        "\n",
        "Training word embeddings requires massive datasets and computational resources. Fortunately, we can use pre-trained embeddings!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "load_embeddings_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c23a1bb-aa9a-4462-e28f-a0b0636b8783"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Loading pre-trained Word2Vec embeddings...\n",
            "‚è≥ This might take a few minutes on first run...\n",
            "‚ö†Ô∏è Could not load embeddings. Using a mock version for demonstration.\n",
            "\n",
            "üìä Embedding Statistics:\n",
            "Using mock embeddings for demonstration\n",
            "\n",
            "üéâ Ready to explore word embeddings!\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained Word2Vec embeddings (this might take a few minutes)\n",
        "print(\"üì• Loading pre-trained Word2Vec embeddings...\")\n",
        "print(\"‚è≥ This might take a few minutes on first run...\")\n",
        "\n",
        "try:\n",
        "    # Load a smaller model for faster loading\n",
        "    word_vectors = api.load('glove-wiki-gigaword-50')  # 50-dimensional GloVe vectors\n",
        "    print(\"‚úÖ Successfully loaded GloVe embeddings!\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è Could not load embeddings. Using a mock version for demonstration.\")\n",
        "    # Create a mock word_vectors object for demonstration\n",
        "    class MockWordVectors:\n",
        "        def __init__(self):\n",
        "            self.vocab = {'king', 'queen', 'man', 'woman', 'movie', 'film', 'good', 'great', 'bad', 'terrible'}\n",
        "\n",
        "        def __contains__(self, word):\n",
        "            return word in self.vocab\n",
        "\n",
        "        def similarity(self, w1, w2):\n",
        "            # Mock similarities\n",
        "            pairs = {('king', 'queen'): 0.8, ('movie', 'film'): 0.9, ('good', 'great'): 0.7}\n",
        "            return pairs.get((w1, w2), pairs.get((w2, w1), 0.3))\n",
        "\n",
        "        def most_similar(self, word, topn=5):\n",
        "            mock_results = {\n",
        "                'king': [('queen', 0.8), ('prince', 0.7), ('royal', 0.6)],\n",
        "                'movie': [('film', 0.9), ('cinema', 0.7), ('theater', 0.6)]\n",
        "            }\n",
        "            return mock_results.get(word, [('similar', 0.5)])\n",
        "\n",
        "    word_vectors = MockWordVectors()\n",
        "\n",
        "print(f\"\\nüìä Embedding Statistics:\")\n",
        "if hasattr(word_vectors, 'vector_size'):\n",
        "    print(f\"Vector dimensions: {word_vectors.vector_size}\")\n",
        "    print(f\"Vocabulary size: {len(word_vectors.key_to_index)}\")\n",
        "else:\n",
        "    print(\"Using mock embeddings for demonstration\")\n",
        "\n",
        "print(\"\\nüéâ Ready to explore word embeddings!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "word_similarity"
      },
      "source": [
        "## üîç Exploring Word Similarities\n",
        "\n",
        "Let's see how word embeddings capture semantic relationships:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "word_similarity_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "979f861d-8709-44c3-98be-376a9a25877f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Word Similarity Exploration:\n",
            "\n",
            "üìä Pairwise Similarities:\n",
            "  movie ‚Üî film: 0.900\n",
            "  good ‚Üî great: 0.700\n",
            "  bad ‚Üî terrible: 0.300\n",
            "  king ‚Üî queen: 0.800\n",
            "  movie ‚Üî king: 0.300\n",
            "  good ‚Üî bad: 0.300\n",
            "\n",
            "üéØ Most Similar Words:\n",
            "\n",
            "'movie' is most similar to:\n",
            "  film: 0.900\n",
            "  cinema: 0.700\n",
            "  theater: 0.600\n",
            "\n",
            "'good' is most similar to:\n",
            "  similar: 0.500\n",
            "\n",
            "'king' is most similar to:\n",
            "  queen: 0.800\n",
            "  prince: 0.700\n",
            "  royal: 0.600\n"
          ]
        }
      ],
      "source": [
        "# Test words for similarity exploration\n",
        "test_words = ['movie', 'film', 'good', 'great', 'bad', 'terrible', 'king', 'queen']\n",
        "\n",
        "print(\"üîç Word Similarity Exploration:\")\n",
        "print(\"\\nüìä Pairwise Similarities:\")\n",
        "\n",
        "# Calculate similarities between word pairs\n",
        "similarity_pairs = [\n",
        "    ('movie', 'film'),\n",
        "    ('good', 'great'),\n",
        "    ('bad', 'terrible'),\n",
        "    ('king', 'queen'),\n",
        "    ('movie', 'king'),  # Should be low\n",
        "    ('good', 'bad')     # Should be low\n",
        "]\n",
        "\n",
        "for word1, word2 in similarity_pairs:\n",
        "    if word1 in word_vectors and word2 in word_vectors:\n",
        "        similarity = word_vectors.similarity(word1, word2)\n",
        "        print(f\"  {word1} ‚Üî {word2}: {similarity:.3f}\")\n",
        "    else:\n",
        "        print(f\"  {word1} ‚Üî {word2}: (not in vocabulary)\")\n",
        "\n",
        "# Find most similar words\n",
        "print(\"\\nüéØ Most Similar Words:\")\n",
        "query_words = ['movie', 'good', 'king']\n",
        "\n",
        "for word in query_words:\n",
        "    if word in word_vectors:\n",
        "        try:\n",
        "            similar_words = word_vectors.most_similar(word, topn=5)\n",
        "            print(f\"\\n'{word}' is most similar to:\")\n",
        "            for similar_word, score in similar_words:\n",
        "                print(f\"  {similar_word}: {score:.3f}\")\n",
        "        except:\n",
        "            print(f\"\\n'{word}': Could not find similar words\")\n",
        "    else:\n",
        "        print(f\"\\n'{word}': Not in vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "word_arithmetic"
      },
      "source": [
        "## üßÆ Word Arithmetic: The Magic of Embeddings\n",
        "\n",
        "One of the most fascinating properties of word embeddings is that they support arithmetic operations that capture semantic relationships!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "word_arithmetic_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c8bd10e-a558-40db-f3eb-f936485ea7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßÆ Word Arithmetic Examples:\n",
            "\n",
            "üîÆ king - man + woman = ?\n",
            "   Expected: queen\n",
            "   Error: MockWordVectors.most_similar() got an unexpected keyword argument 'positive'\n",
            "\n",
            "üîÆ good - bad + terrible = ?\n",
            "   Expected: awful\n",
            "   Error: MockWordVectors.most_similar() got an unexpected keyword argument 'positive'\n",
            "\n",
            "üí° This works because embeddings capture semantic relationships!\n",
            "   The vector from 'man' to 'king' is similar to the vector from 'woman' to 'queen'\n"
          ]
        }
      ],
      "source": [
        "print(\"üßÆ Word Arithmetic Examples:\")\n",
        "\n",
        "# Famous example: king - man + woman ‚âà queen\n",
        "arithmetic_examples = [\n",
        "    ('king', 'man', 'woman', 'queen'),  # king - man + woman = ?\n",
        "    ('good', 'bad', 'terrible', 'awful'),  # good - bad + terrible = ?\n",
        "]\n",
        "\n",
        "for word1, word2, word3, expected in arithmetic_examples:\n",
        "    print(f\"\\nüîÆ {word1} - {word2} + {word3} = ?\")\n",
        "    print(f\"   Expected: {expected}\")\n",
        "\n",
        "    # Check if all words are in vocabulary\n",
        "    if all(word in word_vectors for word in [word1, word2, word3]):\n",
        "        try:\n",
        "            # Perform word arithmetic\n",
        "            if hasattr(word_vectors, 'most_similar'):\n",
        "                result = word_vectors.most_similar(\n",
        "                    positive=[word1, word3],\n",
        "                    negative=[word2],\n",
        "                    topn=3\n",
        "                )\n",
        "                print(\"   Results:\")\n",
        "                for word, score in result:\n",
        "                    print(f\"     {word}: {score:.3f}\")\n",
        "            else:\n",
        "                print(\"   (Mock result: queen: 0.85)\")\n",
        "        except Exception as e:\n",
        "            print(f\"   Error: {e}\")\n",
        "    else:\n",
        "        missing = [w for w in [word1, word2, word3] if w not in word_vectors]\n",
        "        print(f\"   Missing words: {missing}\")\n",
        "\n",
        "print(\"\\nüí° This works because embeddings capture semantic relationships!\")\n",
        "print(\"   The vector from 'man' to 'king' is similar to the vector from 'woman' to 'queen'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise5"
      },
      "source": [
        "### üèãÔ∏è‚Äç‚ôÄÔ∏è **Exercise 5: Embedding Exploration**\n",
        "\n",
        "Explore word embeddings with your own examples:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "def explore_word_relationships(word_vectors, word_list):\n",
        "    \"\"\"\n",
        "    Explore relationships between words using embeddings.\n",
        "    \n",
        "    Args:\n",
        "        word_vectors: Pre-trained word embedding model\n",
        "        word_list (list): List of words to explore\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary with similarity matrix and most similar words\n",
        "    \"\"\"\n",
        "    # TODO: Filter words that exist in the vocabulary\n",
        "    valid_words = [word for word in word_list if word in word_vectors]\n",
        "    \n",
        "    if len(valid_words) < 2:\n",
        "        print(\"Not enough valid words for analysis\")\n",
        "        return None\n",
        "    \n",
        "    print(f\"üìä Analyzing relationships for: {valid_words}\")\n",
        "    \n",
        "    # TODO: Create a similarity matrix\n",
        "    similarity_matrix = []\n",
        "    for word1 in valid_words:\n",
        "        row = []\n",
        "        for word2 in valid_words:\n",
        "            if word1 == word2:\n",
        "                similarity = 1.0\n",
        "            else:\n",
        "                # TODO: Calculate similarity between word1 and word2\n",
        "                similarity =  \n",
        "      \n",
        "            row.append(similarity)\n",
        "        similarity_matrix.append(row)\n",
        "    \n",
        "    # Create DataFrame for visualization\n",
        "    sim_df = pd.DataFrame(similarity_matrix, index=valid_words, columns=valid_words)\n",
        "    \n",
        "    # TODO: Find most similar words for each word\n",
        "    most_similar_dict = {}\n",
        "    for word in valid_words:\n",
        "        try:\n",
        "            # YOUR CODE HERE: Get most similar words\n",
        "            similar = # YOUR CODE HERE\n",
        "            most_similar_dict[word] = similar\n",
        "        except:\n",
        "            most_similar_dict[word] = [(\"unknown\", 0.0)]\n",
        "    \n",
        "    return {\n",
        "        'similarity_matrix': sim_df,\n",
        "        'most_similar': most_similar_dict\n",
        "    }\n",
        "\n",
        "# Test with movie-related words\n",
        "movie_words = ['movie', 'film', 'cinema', 'actor', 'director', 'script', 'good', 'bad']\n",
        "results = explore_word_relationships(word_vectors, movie_words)\n",
        "\n",
        "if results:\n",
        "    print(\"\\nüìä Similarity Matrix:\")\n",
        "    print(results['similarity_matrix'].round(3))\n",
        "    \n",
        "    print(\"\\nüéØ Most Similar Words:\")\n",
        "    for word, similar_list in results['most_similar'].items():\n",
        "        print(f\"\\n{word}:\")\n",
        "        for sim_word, score in similar_list[:3]:\n",
        "            print(f\"  {sim_word}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "exercise5_code"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution5"
      },
      "source": [
        "**üí° Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "solution5_code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e0e4e10-5b16-451e-d9c2-04b467757f2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Solution implemented successfully!\n"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 5\n",
        "def explore_word_relationships_solution(word_vectors, word_list):\n",
        "    # Filter valid words\n",
        "    valid_words = [word for word in word_list if word in word_vectors]\n",
        "\n",
        "    if len(valid_words) < 2:\n",
        "        print(\"Not enough valid words for analysis\")\n",
        "        return None\n",
        "\n",
        "    print(f\"üìä Analyzing relationships for: {valid_words}\")\n",
        "\n",
        "    # Create similarity matrix\n",
        "    similarity_matrix = []\n",
        "    for word1 in valid_words:\n",
        "        row = []\n",
        "        for word2 in valid_words:\n",
        "            if word1 == word2:\n",
        "                similarity = 1.0\n",
        "            else:\n",
        "                similarity = word_vectors.similarity(word1, word2)\n",
        "            row.append(similarity)\n",
        "        similarity_matrix.append(row)\n",
        "\n",
        "    sim_df = pd.DataFrame(similarity_matrix, index=valid_words, columns=valid_words)\n",
        "\n",
        "    # Find most similar words\n",
        "    most_similar_dict = {}\n",
        "    for word in valid_words:\n",
        "        try:\n",
        "            similar = word_vectors.most_similar(word, topn=3)\n",
        "            most_similar_dict[word] = similar\n",
        "        except:\n",
        "            most_similar_dict[word] = [(\"unknown\", 0.0)]\n",
        "\n",
        "    return {\n",
        "        'similarity_matrix': sim_df,\n",
        "        'most_similar': most_similar_dict\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Solution implemented successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sparse_vs_dense"
      },
      "source": [
        "## ‚öñÔ∏è Sparse vs Dense: The Great Comparison\n",
        "\n",
        "Let's compare our sparse representations (BOW, TF-IDF) with dense embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "comparison_table",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "0ccf6380-e097-4587-ced2-b533ffd93d18"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-1582343842.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m }\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mcomparison_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomparison_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚öñÔ∏è Sparse vs Dense Representations Comparison:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomparison_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Create a comparison table\n",
        "comparison_data = {\n",
        "    'Aspect': [\n",
        "        'Dimensionality',\n",
        "        'Sparsity',\n",
        "        'Semantic Understanding',\n",
        "        'Word Order',\n",
        "        'Training Required',\n",
        "        'Interpretability',\n",
        "        'Memory Usage',\n",
        "        'Computation Speed',\n",
        "        'Out-of-Vocabulary Words'\n",
        "    ],\n",
        "    'BOW/TF-IDF (Sparse)': [\n",
        "        'High (vocab size)',\n",
        "        'Very sparse (>95% zeros)',\n",
        "        'Limited',\n",
        "        'Lost (except n-grams)',\n",
        "        'Minimal',\n",
        "        'High (direct word mapping)',\n",
        "        'High (large sparse matrices)',\n",
        "        'Fast for small vocab',\n",
        "        'Easy to handle'\n",
        "    ],\n",
        "    'Word Embeddings (Dense)': [\n",
        "        'Low (50-300 dims)',\n",
        "        'Dense (no zeros)',\n",
        "        'Rich semantic relationships',\n",
        "        'Lost',\n",
        "        'Extensive (large corpus)',\n",
        "        'Low (abstract features)',\n",
        "        'Low (compact vectors)',\n",
        "        'Fast for large vocab',\n",
        "        'Challenging'\n",
        "    ]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(\"‚öñÔ∏è Sparse vs Dense Representations Comparison:\")\n",
        "print(comparison_df.to_string(index=False))\n",
        "\n",
        "# Practical example: vocabulary size comparison\n",
        "print(\"\\nüìä Practical Example - Dimensionality:\")\n",
        "print(f\"Our TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)} dimensions\")\n",
        "if hasattr(word_vectors, 'vector_size'):\n",
        "    print(f\"Word embedding dimensions: {word_vectors.vector_size} dimensions\")\n",
        "    reduction = len(tfidf_vectorizer.vocabulary_) / word_vectors.vector_size\n",
        "    print(f\"Dimensionality reduction: {reduction:.1f}x smaller!\")\n",
        "else:\n",
        "    print(\"Word embedding dimensions: 50 dimensions (typical)\")\n",
        "    reduction = len(tfidf_vectorizer.vocabulary_) / 50\n",
        "    print(f\"Dimensionality reduction: {reduction:.1f}x smaller!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reflection4"
      },
      "source": [
        "### ü§î **Reflection Questions - Part 4**\n",
        "\n",
        "**Question 1:** Explain the distributional hypothesis in your own words. Why is it important for word embeddings?\n",
        "\n",
        "**Your Answer:**\n",
        "The distributional hypothesis says that words appearing in similar contexts tend to have smaller meanings. This idea is the foundation for word embeddings, which turn words into vectors based on how they're used in text. It helps models understand that words like \"king\" and \"queen\" are related, even if they're not directly defined.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2:** Why does \"king - man + woman ‚âà queen\" work in word embeddings? What does this tell us about the vector space?\n",
        "\n",
        "**Your Answer:**\n",
        "The model captures the relationship between words as direction in space. It's not just storing meanings - it's learning how words relate to each other. This tells us vector has structure, where semantic relationships become math - so changing \"man\" to \"woman\" shifts \"king\" in the same direction that leads to \"queen\".\n",
        "\n",
        "---\n",
        "\n",
        "**Question 3:** Based on the comparison table, when would you choose sparse representations over dense embeddings?\n",
        "\n",
        "**Your Answer:**\n",
        "You'd choose sparse representations (like Bag of Words or TF-IDF) when your data is small, you want simple models, or you need speed and transparency- like in basic text classification or quick filters. They're easier to understand and faster to compute, especially when deep meaning isn't nessesary. But if you are dealing with large datasets, complex language patterns, or want deeper context (like for chatbots or semantic search) then dense embeddings are the way to go.  So it's a trade off: sparce is simple and light, dense is smart and powerful.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4:** What are the potential ethical concerns with word embeddings? (Hint: think about bias in training data)\n",
        "\n",
        "**Your Answer:**\n",
        "Word embeddings can pick up bias from the training data, because they learn from real world text - which often include stereotypes, racism, sexism, and other harmful patterns. For example, an embedding might associate \"man\" with \"doctor\" and \"woman\" with \"nurse\" just because that's what it saw in the data. This can lead to biased predictions in AI systems, like unfair hiring tools or toxic chat bots. So the concern is if the data's biased, the model will be too. That can mess up real peoples lives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "day5_header"
      },
      "source": [
        "# üìÖ Part 5: Integration & Real-World Applications\n",
        "\n",
        "## üéØ **Part  5 Goals:**\n",
        "- Build a complete text classification system\n",
        "- Compare all representation methods on a real task\n",
        "- Explore real-world applications\n",
        "- Reflect on ethical considerations\n",
        "\n",
        "## üèóÔ∏è Building a Text Classification System\n",
        "\n",
        "Let's put everything together and build a movie review sentiment classifier using different text representations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "larger_dataset"
      },
      "source": [
        "### üìö Loading a Larger Dataset\n",
        "\n",
        "First, let's get a more substantial dataset for our classification task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "load_dataset",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "9be52bea-af2c-40ac-c8b9-6a7a6c47ad3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìö Loading movie reviews dataset...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'movie_reviews' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-27-3667917222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get positive and negative reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpositive_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmovie_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mnegative_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmovie_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmovie_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'movie_reviews' is not defined"
          ]
        }
      ],
      "source": [
        "# Load movie reviews dataset from NLTK\n",
        "print(\"üìö Loading movie reviews dataset...\")\n",
        "\n",
        "# Get positive and negative reviews\n",
        "positive_reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids('pos')]\n",
        "negative_reviews = [movie_reviews.raw(fileid) for fileid in movie_reviews.fileids('neg')]\n",
        "\n",
        "# Combine and create labels\n",
        "all_reviews = positive_reviews + negative_reviews\n",
        "all_labels = [1] * len(positive_reviews) + [0] * len(negative_reviews)\n",
        "\n",
        "print(f\"üìä Dataset Statistics:\")\n",
        "print(f\"Total reviews: {len(all_reviews)}\")\n",
        "print(f\"Positive reviews: {len(positive_reviews)}\")\n",
        "print(f\"Negative reviews: {len(negative_reviews)}\")\n",
        "\n",
        "# Take a subset for faster processing (adjust size based on your computational resources)\n",
        "subset_size = min(200, len(all_reviews))  # Use 200 reviews or all if less\n",
        "reviews_subset = all_reviews[:subset_size]\n",
        "labels_subset = all_labels[:subset_size]\n",
        "\n",
        "print(f\"\\nüéØ Using subset of {len(reviews_subset)} reviews for analysis\")\n",
        "\n",
        "# Show example reviews\n",
        "print(\"\\nüìù Example Reviews:\")\n",
        "for i in range(2):\n",
        "    sentiment = \"üòä Positive\" if labels_subset[i] == 1 else \"üòû Negative\"\n",
        "    print(f\"\\n{i+1}. [{sentiment}] {reviews_subset[i][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "classification_pipeline"
      },
      "source": [
        "### üîß Building Classification Pipelines\n",
        "\n",
        "Let's create classification pipelines using different text representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "classification_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "a086e378-2ef0-4443-d13f-fb679af9351e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-271125019.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mreviews_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_subset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    reviews_subset, labels_subset, test_size=0.3, random_state=42, stratify=labels_subset\n",
        ")\n",
        "\n",
        "print(f\"üìä Data Split:\")\n",
        "print(f\"Training set: {len(X_train)} reviews\")\n",
        "print(f\"Test set: {len(X_test)} reviews\")\n",
        "\n",
        "# Initialize results dictionary\n",
        "results = {}\n",
        "\n",
        "# 1. BOW Classification\n",
        "print(\"\\nüéí Training BOW Classifier...\")\n",
        "bow_vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
        "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
        "X_test_bow = bow_vectorizer.transform(X_test)\n",
        "\n",
        "bow_classifier = MultinomialNB()\n",
        "bow_classifier.fit(X_train_bow, y_train)\n",
        "bow_predictions = bow_classifier.predict(X_test_bow)\n",
        "bow_accuracy = accuracy_score(y_test, bow_predictions)\n",
        "\n",
        "results['BOW'] = {\n",
        "    'accuracy': bow_accuracy,\n",
        "    'predictions': bow_predictions,\n",
        "    'features': X_train_bow.shape[1]\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ BOW Accuracy: {bow_accuracy:.3f}\")\n",
        "\n",
        "# 2. TF-IDF Classification\n",
        "print(\"\\nüî• Training TF-IDF Classifier...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "tfidf_classifier = MultinomialNB()\n",
        "tfidf_classifier.fit(X_train_tfidf, y_train)\n",
        "tfidf_predictions = tfidf_classifier.predict(X_test_tfidf)\n",
        "tfidf_accuracy = accuracy_score(y_test, tfidf_predictions)\n",
        "\n",
        "results['TF-IDF'] = {\n",
        "    'accuracy': tfidf_accuracy,\n",
        "    'predictions': tfidf_predictions,\n",
        "    'features': X_train_tfidf.shape[1]\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ TF-IDF Accuracy: {tfidf_accuracy:.3f}\")\n",
        "\n",
        "# 3. N-gram Classification\n",
        "print(\"\\nüîó Training N-gram Classifier...\")\n",
        "ngram_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 2))\n",
        "X_train_ngram = ngram_vectorizer.fit_transform(X_train)\n",
        "X_test_ngram = ngram_vectorizer.transform(X_test)\n",
        "\n",
        "ngram_classifier = MultinomialNB()\n",
        "ngram_classifier.fit(X_train_ngram, y_train)\n",
        "ngram_predictions = ngram_classifier.predict(X_test_ngram)\n",
        "ngram_accuracy = accuracy_score(y_test, ngram_predictions)\n",
        "\n",
        "results['N-grams'] = {\n",
        "    'accuracy': ngram_accuracy,\n",
        "    'predictions': ngram_predictions,\n",
        "    'features': X_train_ngram.shape[1]\n",
        "}\n",
        "\n",
        "print(f\"‚úÖ N-grams Accuracy: {ngram_accuracy:.3f}\")\n",
        "\n",
        "print(\"\\nüéâ All classifiers trained successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_comparison"
      },
      "source": [
        "### üìä Comparing Results\n",
        "\n",
        "Let's visualize and compare the performance of different methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "results_viz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "1fe1161b-10d2-404d-ddf5-86c8941451e3"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-29-1517377585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create results DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m results_df = pd.DataFrame({\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m'Method'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'Features'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    'Method': list(results.keys()),\n",
        "    'Accuracy': [results[method]['accuracy'] for method in results.keys()],\n",
        "    'Features': [results[method]['features'] for method in results.keys()]\n",
        "})\n",
        "\n",
        "print(\"üìä Classification Results Comparison:\")\n",
        "print(results_df.round(3))\n",
        "\n",
        "# Visualize results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Accuracy comparison\n",
        "bars1 = ax1.bar(results_df['Method'], results_df['Accuracy'],\n",
        "                color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "ax1.set_title('üéØ Classification Accuracy Comparison')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.set_ylim(0, 1)\n",
        "\n",
        "# Add accuracy values on bars\n",
        "for bar, acc in zip(bars1, results_df['Accuracy']):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# Feature count comparison\n",
        "bars2 = ax2.bar(results_df['Method'], results_df['Features'],\n",
        "                color=['skyblue', 'lightcoral', 'lightgreen'])\n",
        "ax2.set_title('üìè Feature Count Comparison')\n",
        "ax2.set_ylabel('Number of Features')\n",
        "\n",
        "# Add feature counts on bars\n",
        "for bar, feat in zip(bars2, results_df['Features']):\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
        "             f'{feat}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification reports\n",
        "print(\"\\nüìã Detailed Classification Reports:\")\n",
        "for method in results.keys():\n",
        "    print(f\"\\n{method} Classification Report:\")\n",
        "    print(classification_report(y_test, results[method]['predictions'],\n",
        "                              target_names=['Negative', 'Positive']))\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise6"
      },
      "source": [
        "### üèãÔ∏è‚Äç‚ôÄÔ∏è **Exercise 6: Feature Analysis**\n",
        "\n",
        "Analyze which features (words) are most important for classification:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "exercise6_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "42b5f27a-070f-4a3d-b2ba-b8137c08ba7a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-30-3158367388.py, line 20)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-30-3158367388.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    coef = # YOUR CODE HERE\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "def analyze_important_features(vectorizer, classifier, top_n=10):\n",
        "    \"\"\"\n",
        "    Analyze the most important features for classification.\n",
        "\n",
        "    Args:\n",
        "        vectorizer: Fitted vectorizer (CountVectorizer or TfidfVectorizer)\n",
        "        classifier: Fitted classifier\n",
        "        top_n (int): Number of top features to return\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with positive and negative features\n",
        "    \"\"\"\n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    # TODO: Get feature coefficients from the classifier\n",
        "    # Hint: For Naive Bayes, use classifier.feature_log_prob_\n",
        "    if hasattr(classifier, 'feature_log_prob_'):\n",
        "        # For Naive Bayes: difference between positive and negative class probabilities\n",
        "        coef = classifier.feature_log_prob_[1] - classifier.feature_log_prob_[0]\n",
        "    else:\n",
        "        # For linear classifiers: use coef_ attribute\n",
        "        coef = classifier.coef_[0]\n",
        "\n",
        "    # TODO: Get indices of top positive and negative features\n",
        "    top_positive_indices =\n",
        "    top_negative_indices =\n",
        "\n",
        "    # TODO: Get the actual feature names and their scores\n",
        "    positive_features = [(feature_names[i], coef[i]) for i in top_positive_indices]\n",
        "    negative_features = [(feature_names[i], coef[i]) for i in top_negative_indices]\n",
        "\n",
        "    return {\n",
        "        'positive': positive_features,\n",
        "        'negative': negative_features\n",
        "    }\n",
        "\n",
        "# Analyze TF-IDF features\n",
        "print(\"üîç Most Important Features for TF-IDF Classifier:\")\n",
        "important_features = analyze_important_features(tfidf_vectorizer, tfidf_classifier, top_n=10)\n",
        "\n",
        "print(\"\\nüòä Top Positive Features (indicate positive sentiment):\")\n",
        "for feature, score in important_features['positive']:\n",
        "    print(f\"  {feature}: {score:.3f}\")\n",
        "\n",
        "print(\"\\nüòû Top Negative Features (indicate negative sentiment):\")\n",
        "for feature, score in important_features['negative']:\n",
        "    print(f\"  {feature}: {score:.3f}\")\n",
        "\n",
        "# Visualize feature importance\n",
        "pos_features, pos_scores = zip(*important_features['positive'])\n",
        "neg_features, neg_scores = zip(*important_features['negative'])\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "ax1.barh(pos_features, pos_scores, color='green', alpha=0.7)\n",
        "ax1.set_title('üòä Top Positive Features')\n",
        "ax1.set_xlabel('Feature Importance')\n",
        "\n",
        "ax2.barh(neg_features, neg_scores, color='red', alpha=0.7)\n",
        "ax2.set_title('üòû Top Negative Features')\n",
        "ax2.set_xlabel('Feature Importance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solution6"
      },
      "source": [
        "**üí° Solution Check:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "solution6_code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "afb37ded-0115-48d1-bfcb-6ec225eec9e0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tfidf_vectorizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-31-1425428972.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Test solution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0msolution_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_important_features_solution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ Solution - Top 5 positive features:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msolution_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tfidf_vectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Solution for Exercise 6\n",
        "def analyze_important_features_solution(vectorizer, classifier, top_n=10):\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    if hasattr(classifier, 'feature_log_prob_'):\n",
        "        # For Naive Bayes: difference between positive and negative class log probabilities\n",
        "        coef = classifier.feature_log_prob_[1] - classifier.feature_log_prob_[0]\n",
        "    else:\n",
        "        coef = classifier.coef_[0]\n",
        "\n",
        "    # Get top positive and negative features\n",
        "    top_positive_indices = np.argsort(coef)[-top_n:]\n",
        "    top_negative_indices = np.argsort(coef)[:top_n]\n",
        "\n",
        "    positive_features = [(feature_names[i], coef[i]) for i in reversed(top_positive_indices)]\n",
        "    negative_features = [(feature_names[i], coef[i]) for i in top_negative_indices]\n",
        "\n",
        "    return {\n",
        "        'positive': positive_features,\n",
        "        'negative': negative_features\n",
        "    }\n",
        "\n",
        "# Test solution\n",
        "solution_features = analyze_important_features_solution(tfidf_vectorizer, tfidf_classifier, 5)\n",
        "print(\"‚úÖ Solution - Top 5 positive features:\")\n",
        "for feature, score in solution_features['positive']:\n",
        "    print(f\"  {feature}: {score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "real_world_apps"
      },
      "source": [
        "## üåç Real-World Applications\n",
        "\n",
        "Let's explore how text representation techniques are used in real-world applications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "applications_demo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c541181e-2882-4e86-e8da-3fc9fdace0da"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-1718591372.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m }\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mapps_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üåç Real-World Applications of Text Representation:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapps_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Create a comprehensive overview of real-world applications\n",
        "applications = {\n",
        "    'Application': [\n",
        "        'Search Engines',\n",
        "        'Recommendation Systems',\n",
        "        'Sentiment Analysis',\n",
        "        'Machine Translation',\n",
        "        'Chatbots & Virtual Assistants',\n",
        "        'Document Classification',\n",
        "        'Spam Detection',\n",
        "        'Content Moderation',\n",
        "        'News Categorization',\n",
        "        'Medical Text Analysis'\n",
        "    ],\n",
        "    'Text Representation Used': [\n",
        "        'TF-IDF, Word Embeddings',\n",
        "        'Word Embeddings, Collaborative Filtering',\n",
        "        'TF-IDF, N-grams, Embeddings',\n",
        "        'Word Embeddings, Contextual Embeddings',\n",
        "        'Word Embeddings, Contextual Models',\n",
        "        'TF-IDF, BOW, Embeddings',\n",
        "        'TF-IDF, N-grams',\n",
        "        'TF-IDF, Embeddings, Deep Learning',\n",
        "        'TF-IDF, Topic Models',\n",
        "        'Domain-specific Embeddings, TF-IDF'\n",
        "    ],\n",
        "    'Key Challenge': [\n",
        "        'Relevance ranking, query understanding',\n",
        "        'Cold start problem, scalability',\n",
        "        'Sarcasm, context, domain adaptation',\n",
        "        'Preserving meaning, handling idioms',\n",
        "        'Context understanding, dialogue flow',\n",
        "        'Class imbalance, feature selection',\n",
        "        'Adversarial attacks, evolving spam',\n",
        "        'Bias, cultural sensitivity, scale',\n",
        "        'Real-time processing, topic drift',\n",
        "        'Privacy, specialized terminology'\n",
        "    ]\n",
        "}\n",
        "\n",
        "apps_df = pd.DataFrame(applications)\n",
        "print(\"üåç Real-World Applications of Text Representation:\")\n",
        "print(apps_df.to_string(index=False))\n",
        "\n",
        "# Demonstrate a simple search engine using TF-IDF\n",
        "print(\"\\nüîç Mini Search Engine Demo:\")\n",
        "\n",
        "def simple_search_engine(documents, query, top_k=3):\n",
        "    \"\"\"\n",
        "    Simple search engine using TF-IDF similarity.\n",
        "    \"\"\"\n",
        "    # Create TF-IDF vectors for documents and query\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    doc_vectors = vectorizer.fit_transform(documents)\n",
        "    query_vector = vectorizer.transform([query])\n",
        "\n",
        "    # Calculate similarities\n",
        "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
        "\n",
        "    # Get top results\n",
        "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    for i, idx in enumerate(top_indices):\n",
        "        results.append({\n",
        "            'rank': i + 1,\n",
        "            'document': documents[idx][:100] + \"...\",\n",
        "            'similarity': similarities[idx]\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "# Demo with our movie reviews\n",
        "search_query = \"great acting performance\"\n",
        "search_results = simple_search_engine(reviews_subset[:20], search_query)\n",
        "\n",
        "print(f\"\\nQuery: '{search_query}'\")\n",
        "print(\"\\nTop 3 Results:\")\n",
        "for result in search_results:\n",
        "    print(f\"\\n{result['rank']}. Similarity: {result['similarity']:.3f}\")\n",
        "    print(f\"   {result['document']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethical_considerations"
      },
      "source": [
        "## ‚öñÔ∏è Ethical Considerations\n",
        "\n",
        "As we've learned about text representation, it's crucial to understand the ethical implications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ethics_demo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "ec355388-ff2e-42e6-8963-03866b2ed8be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öñÔ∏è Ethical Considerations in Text Representation:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-1922465609.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m }\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0methics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0methical_issues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0methics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"‚öñÔ∏è Ethical Considerations in Text Representation:\")\n",
        "\n",
        "ethical_issues = {\n",
        "    'Issue': [\n",
        "        'Bias in Training Data',\n",
        "        'Representation Bias',\n",
        "        'Privacy Concerns',\n",
        "        'Fairness in Applications',\n",
        "        'Transparency',\n",
        "        'Cultural Sensitivity'\n",
        "    ],\n",
        "    'Description': [\n",
        "        'Word embeddings reflect societal biases present in training text',\n",
        "        'Underrepresentation of certain groups in training data',\n",
        "        'Text data may contain sensitive personal information',\n",
        "        'Biased representations can lead to unfair treatment',\n",
        "        'Complex embeddings are difficult to interpret and explain',\n",
        "        'Models may not work well across different cultures/languages'\n",
        "    ],\n",
        "    'Example': [\n",
        "        '\"doctor\" closer to \"man\", \"nurse\" closer to \"woman\"',\n",
        "        'Fewer examples of minority group language patterns',\n",
        "        'Personal emails, medical records in training data',\n",
        "        'Biased hiring algorithms, unfair loan decisions',\n",
        "        'Cannot explain why certain decisions were made',\n",
        "        'English-centric models failing on other languages'\n",
        "    ],\n",
        "    'Mitigation Strategy': [\n",
        "        'Bias detection, debiasing techniques, diverse training data',\n",
        "        'Inclusive data collection, balanced representation',\n",
        "        'Data anonymization, privacy-preserving techniques',\n",
        "        'Fairness metrics, bias testing, diverse teams',\n",
        "        'Interpretable models, explanation techniques',\n",
        "        'Multilingual models, cultural adaptation'\n",
        "    ]\n",
        "}\n",
        "\n",
        "ethics_df = pd.DataFrame(ethical_issues)\n",
        "print(ethics_df.to_string(index=False))\n",
        "\n",
        "# Demonstrate bias detection (conceptual example)\n",
        "print(\"\\nüîç Bias Detection Example:\")\n",
        "print(\"If we had access to large word embeddings, we might find:\")\n",
        "print(\"‚Ä¢ 'programmer' + 'woman' ‚â† 'female programmer' (as expected)\")\n",
        "print(\"‚Ä¢ 'doctor' might be closer to 'he' than 'she'\")\n",
        "print(\"‚Ä¢ Certain ethnic names might cluster away from positive adjectives\")\n",
        "print(\"\\nüí° This is why bias testing and mitigation are crucial!\")\n",
        "\n",
        "print(\"\\nüéØ Best Practices for Ethical Text Representation:\")\n",
        "best_practices = [\n",
        "    \"1. Audit training data for bias and representation gaps\",\n",
        "    \"2. Test models across different demographic groups\",\n",
        "    \"3. Use diverse teams in model development and evaluation\",\n",
        "    \"4. Implement bias detection and mitigation techniques\",\n",
        "    \"5. Provide transparency about model limitations\",\n",
        "    \"6. Regular monitoring and updating of deployed models\",\n",
        "    \"7. Consider cultural and linguistic diversity\",\n",
        "    \"8. Respect privacy and obtain proper consent for data use\"\n",
        "]\n",
        "\n",
        "for practice in best_practices:\n",
        "    print(practice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "final_reflection"
      },
      "source": [
        "### ü§î **Final Reflection Questions -Part 5**\n",
        "\n",
        "**Question 1:** Based on your classification results, which text representation method performed best? Why do you think this is the case?\n",
        "\n",
        "**Your Answer:**\n",
        "BoW performed decently but struggled with context, TF-IDF performmed better by down wieghing common words and highlighting more meaningful terms and word embeddings performed the best because it captures the meaning and realtionship between words.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 2:** Describe a real-world application where you would use each of the following:\n",
        "- BOW representation:\n",
        "- TF-IDF representation:\n",
        "- Word embeddings:\n",
        "\n",
        "**Your Answer:**\n",
        "BoW- Used for quick spam email detection, TF-IDF is great for document search engines to rank results by highlighting important, unique words in each document, and word embeddings are perfect for chat bots that need to understand the meaning behind user messages and respod naturally.  \n",
        "\n",
        "---\n",
        "\n",
        "**Question 3:** What ethical considerations should be taken into account when deploying a text classification system in a real-world application (e.g., resume screening, content moderation)?\n",
        "\n",
        "**Your Answer:**\n",
        "When deploying a tax classification system, you have to watch for bias in the training data that could unfairly descriminate against certain groups. Make sure the system is transparent and explainable, so decisions can be understood and challenged. Protect users privacy by handlong sensitive data carefully and securely. Always handle human over sight to catch mistakes and avoid unfair automated judgements.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 4:** How has your understanding of text representation evolved over these 5 parts? What was the most surprising thing you learned?\n",
        "\n",
        "**Your Answer:**\n",
        "My understanding grew from simple word counts to seeing how context and meaning matter with embeddings; the most suprising part was how math in vector spaace can capture complex word relationships like \"king-man + woman = queen.\n",
        "\n",
        "---\n",
        "\n",
        "**Question 5:** If you were to continue learning about text representation, what topics would you want to explore next?\n",
        "\n",
        "**Your Answer:**\n",
        "I would like to explore transformers, attantion mechanisms and how large language models undertsand and generate text. They're the cutting edge tech powering today's smartest AI language tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "# üéâ Congratulations! You've Completed Your Text Representation Journey!\n",
        "\n",
        "## üèÜ **What You've Accomplished:**\n",
        "\n",
        "On each one of the 5 parts, you've mastered the fundamental concepts of text representation:\n",
        "\n",
        "### üìö **Technical Skills Gained:**\n",
        "- ‚úÖ Text preprocessing and tokenization\n",
        "- ‚úÖ Bag of Words (BOW) implementation from scratch\n",
        "- ‚úÖ TF-IDF calculation and application\n",
        "- ‚úÖ N-gram analysis for capturing word sequences\n",
        "- ‚úÖ Word embeddings exploration and semantic analysis\n",
        "- ‚úÖ Document similarity using cosine similarity\n",
        "- ‚úÖ Complete text classification pipeline\n",
        "- ‚úÖ Feature importance analysis\n",
        "\n",
        "### üß† **Conceptual Understanding:**\n",
        "- ‚úÖ Why computers need numerical representations of text\n",
        "- ‚úÖ Evolution from sparse to dense representations\n",
        "- ‚úÖ Trade-offs between different representation methods\n",
        "- ‚úÖ Real-world applications and use cases\n",
        "- ‚úÖ Ethical considerations and bias in text representations\n",
        "\n",
        "### üîß **Practical Experience:**\n",
        "- ‚úÖ Working with real datasets (movie reviews)\n",
        "- ‚úÖ Using professional libraries (scikit-learn, gensim)\n",
        "- ‚úÖ Building and evaluating machine learning models\n",
        "- ‚úÖ Comparing different approaches systematically\n",
        "- ‚úÖ Visualizing and interpreting results\n",
        "\n",
        "\n",
        "\n",
        "## üìù **Submission Checklist:**\n",
        "\n",
        "Before submitting your notebook, ensure you have:\n",
        "\n",
        "- [ ] Completed all exercises (1-6)\n",
        "- [ ] Answered all reflection questions\n",
        "- [ ] Run all code cells and verified outputs are visible\n",
        "- [ ] Provided thoughtful analysis of your results\n",
        "- [ ] Discussed ethical considerations\n",
        "- [ ] Saved your notebook with the proper file name   L04_Your_fullname_ITAI_2373.ipynb  or L04_Your_fullname_ITAI_2373.pdf\n",
        "\n",
        "## üåü **Final Words:**\n",
        "\n",
        "Text representation is the foundation of modern NLP and AI systems. The concepts you've learned here are used in everything from search engines to chatbots, from recommendation systems to language translation tools. You've taken the first crucial steps into the exciting world of Natural Language Processing!\n",
        "\n",
        "Remember: *\"The best way to learn is by doing, and you've done an amazing job!\"* üéì\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for your dedication and curiosity. Keep exploring, keep learning, and keep building amazing things with text and AI!** üöÄ‚ú®"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}